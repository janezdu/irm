{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 1,
>>>>>>> be751cb8b7370bd3384ef4e527b6d786efd1e562
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
<<<<<<< HEAD
    "import torch.nn as nn \n",
=======
    "import torch.nn as nn\n",
>>>>>>> be751cb8b7370bd3384ef4e527b6d786efd1e562
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notes\n",
    "- aim for a shared phi\n",
    "- different classifiers for each environment, w_e depends on e\n",
    "2 phases, sort of:\n",
    "1. minimize the sum across all environments, the expected loss with env-specific weight vectors\n",
    "2. fix the phi from step 1. learn a w which is shared among the environments\n",
    "\n",
    "Why do we want that?\n",
    "- from the DARE paper (Rosenfeld '22): sharing phi for environments is okay. \n",
    "    - even if you \"cheat\" when you create the featurizer by looking at test env, it doesn't improve the perf. that much\n",
    "- the bottleneck is actually in learning a _robust w_ .\n",
    "\n",
    "1. `learn_featurizer` = small neural network, calculate loss carefully\n",
    "2. `learn_classifier` = calculate average loss\n",
    "\n",
    "## datasets\n",
    "- Waterbirds, from Sagawa '22\n",
    "    - https://github.com/kohpangwei/group_DRO\n",
    "    - DARE, ISR used\n",
    "\n",
    "\n",
    "How does it work\n",
    "- put in a NN with whatever layers, and a last one\n",
    "- first train neural network\n",
    "- loss is sum with different w's on different environments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# celebA\n",
    "H, W = 218, 178\n",
    "\n",
    "#waterbirds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8dd41e097c85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mBasicNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         '''\n\u001b[0;32m      4\u001b[0m             \u001b[0mInitialize\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCAFENet\u001b[0m \u001b[0mby\u001b[0m \u001b[0mcalling\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msuperclass\u001b[0m\u001b[0;31m'\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0minitializing\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlinear\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(net, X, y, n_epochs=201):\n",
    "    # stepsize = 0.1\n",
    "    # optimizer = torch.optim.Adam(net.parameters())\n",
    "    losses = []\n",
    "    w = torch.zeros(100, requires_grad=True)\n",
    "    # netparams = \n",
    "\n",
    "    for k in range(n_epochs):\n",
    "\n",
    "\n",
    "        alpha_y = alpha * y_train.reshape(-1)\n",
    "        alpha.retain_grad()\n",
    "        f = 0.5 * alpha_y.T @ K @ alpha_y - torch.sum(alpha)\n",
    "        f.backward()\n",
    "        with torch.no_grad():\n",
    "            alpha = (alpha - lr * alpha.grad).clamp(0,c)\n",
    "\n",
    "    \n",
    "    return losses\n",
    "    \n",
    "net = BasicNet()\n",
    "\n",
    "losses = fit(net, X, y)\n",
    "print(losses[0], \"first\")\n",
    "print(losses[-1], \"last\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cccc82ffc9a049ec64036eaf2b5ea3724e12af2330368d7385e76b1e93e5a5db"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
